# Cost Analysis & Optimization Strategy

This document outlines the pricing model for the AI services and strategies used to minimize costs while maintaining high-quality observability.

## Gemini 1.5 Flash Pricing (Estimated)

We use `gemini-1.5-flash` for high-performance, low-latency log analysis.

| Metric | Cost (per 1,000,000 tokens) |
| :--- | :--- |
| **Input Tokens** | $0.075 |
| **Output Tokens** | $0.300 |

### Typical Request Breakdown
*   **Average Prompt length**: ~1,500 tokens (includes schema context and sample logs)
*   **Average Response length**: ~200 tokens
*   **Cost per Request**: ~$0.00017
*   **Requests per $1.00**: ~5,800 requests

### 1. Granular Token Tracking
We track **Input Tokens** and **Output Tokens** separately in the `AICache` collection.
*   **Input Tokens**: Tokens sent to the model (Prompts + Context).
*   **Output Tokens**: Tokens generated by the model.
*   **Cost Calculation**: Total cost is calculated dynamically in the API layer based on current Gemini pricing logic ($0.075/1M input, $0.30/1M output). This ensures statistics always reflect the latest pricing without needing database migrations.

### 2. Multi-Level Caching
We implement a `base64` hash-based caching system in `AICache`. 
... (rest of the content)

### 2. Prompt Summarization
Instead of sending every raw log entry to the AI, we pre-summarize data in the backend:
*   Aggregate status codes.
*   Calculate average latency.
*   Send only a subset of relevant log entries (e.g., the top 5 slowest).

### 3. Contextual Querying
The assistant first generates a MongoDB query, executes it, and *then* processes only the relevant data. This prevents expensive "full database" context windows.

## Estimated Cost vs. Value

| Tier | Volume (Log Analysis) | Est. Monthly Cost |
| :--- | :--- | :--- |
| **Developer** | 100 AI chats/day | ~$0.50 |
| **Professional** | 1,000 AI chats/day | ~$5.00 |
| **Enterprise** | 10,000 AI chats/day | ~$50.00 |
